{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires pymongo 3.6.0+\n",
    "from pymongo import MongoClient\n",
    "from bson.code import Code\n",
    "\n",
    "client = MongoClient(\"mongodb://bigdata-mongodb-04.virtual.uniandes.edu.co:8087/\", retryWrites=False)\n",
    "database = client[\"Grupo03\"]\n",
    "collection = database[\"COL_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = Code(\"function () {\"\n",
    "            'var regex = /[!\"#$%&' + \"'()*+,-./:;<=>?[\\]^_`{|}~]/g;\"\n",
    "            \"var palabras = this.reply_or_quote.split(' ');\"       \n",
    "            \"palabras.forEach(function(z) {\"\n",
    "            \"z = z.replace(regex, '');\"\n",
    "            \"z = z.replace(/\\\\n/g,\"\");\"\n",
    "            \"emit(z.toLowerCase(), 1);\"\n",
    "            \"});\"\n",
    "            \"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce = Code(\"function (key, values) {\"\n",
    "               \"  var total = 0;\"\n",
    "               \"  for (var i = 0; i < values.length; i++) {\"\n",
    "               \"    total += values[i];\"\n",
    "               \"  }\"\n",
    "               \"  return total;\"\n",
    "               \"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contador de palabras / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = database.COL_dataset.map_reduce(map, reduce, \"emociones\")\n",
    "#for doc in result.find():\n",
    "#    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('spanish')) \n",
    "\n",
    "collection = database[\"emociones\"]\n",
    "query = {}\n",
    "query[\"_id\"] = {\n",
    "    u\"$exists\": True\n",
    "}\n",
    "\n",
    "projection = {}\n",
    "projection[\"_id\"] = 1.0\n",
    "stopwords = {'q', 'si', 'usted', 'tan', 'solo', 'ser', 'bien', 'as칤', 'mas', 'va', 'van', 'se침or', 'hace', 'hacer', \n",
    "             'siempre', 'gracias', 'favor', 'puede'}\n",
    "stop_words.update(stopwords)\n",
    "\n",
    "cursor = collection.find(query, projection = projection)\n",
    "try:\n",
    "    for doc in cursor:\n",
    "        if (doc['_id'] in stop_words or (doc['_id'][0:1] == '@')):\n",
    "            collection.delete_one({'_id': doc['_id']})\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "collection = database[\"COL_dataset\"]\n",
    "\n",
    "# Created with Studio 3T, the IDE for MongoDB - https://studio3t.com/\n",
    "\n",
    "query = {}\n",
    "projection = {}\n",
    "projection[\"reply_or_quote\"] = 1.0\n",
    "\n",
    "data = []\n",
    "cursor = collection.find(query, projection = projection)\n",
    "try:\n",
    "    for doc in cursor:\n",
    "        data.append([doc['reply_or_quote']])\n",
    "finally:\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@ClaudiaLopez @infopresidencia @Bogota Para cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@ClaudiaLopez @infopresidencia @Bogota La verd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@ClaudiaLopez @infopresidencia @Bogota Pero, x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@ClaudiaLopez @infopresidencia @Bogota Ingreso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@ClaudiaLopez @infopresidencia @Bogota Estimad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @ClaudiaLopez @infopresidencia @Bogota Para cu...\n",
       "1  @ClaudiaLopez @infopresidencia @Bogota La verd...\n",
       "2  @ClaudiaLopez @infopresidencia @Bogota Pero, x...\n",
       "3  @ClaudiaLopez @infopresidencia @Bogota Ingreso...\n",
       "4  @ClaudiaLopez @infopresidencia @Bogota Estimad..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data,columns=['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (df['text'] == '@')):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_top_n_words(corpus, n=1,k=1):\n",
    "    vec = CountVectorizer(ngram_range=(k,k),stop_words = stop_words).fit(corpus)\n",
    "    print(vec)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(3, 3), preprocessor=None,\n",
      "                stop_words={'a', 'al', 'algo', 'algunas', 'algunos', 'ante',\n",
      "                            'antes', 'as칤', 'bien', 'como', 'con', 'contra',\n",
      "                            'cual', 'cuando', 'de', 'del', 'desde', 'donde',\n",
      "                            'durante', 'e', 'el', 'ella', 'ellas', 'ellos',\n",
      "                            'en', 'entre', 'era', 'erais', 'eran', 'eras', ...},\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "claudialopez infopresidencia bogota 56\n",
      "trabajar trabajar trabajar 26\n",
      "revistasemana claudialopez revistadinero 24\n",
      "claudialopez se침ora alcaldesa 21\n",
      "bluradioco claudialopez vieja 19\n",
      "petrogustavo mucha fuerza 16\n",
      "petrogustavo fuerza petro 15\n",
      "bla bla bla 14\n",
      "bluradioco claudialopez se침ora 14\n",
      "mabellaranews dnp_colombia mabel 14\n",
      "revistasemana vickydavilah gustavobolivar 13\n",
      "mabellaranews alpina urosario 13\n",
      "petrogustavo pronta recuperaci칩n 13\n",
      "claudialopez sra alcaldesa 11\n",
      "치lvaro uribe v칠lez 11\n",
      "mabellaranews aislamiento inteligente 11\n",
      "alvarouribevel grande presidente 11\n",
      "agro ingreso seguro 10\n",
      "luis alberto rodr칤guez 10\n",
      "alvarouribevel grande uribe 9\n"
     ]
    }
   ],
   "source": [
    "common_words = get_top_n_words(df['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pos = ['nc00000', 'nc0n000', 'nc0p000', 'nc0s000', 'np00000']\n",
    "\n",
    "def ExtractInteresting(sentence, good):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    interesting = [k for k,v in nltk.pos_tag(words) if v in good]\n",
    "    print(interesting)\n",
    "    return(interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " verdad\n",
      "verdad ayuda\n",
      "ayuda mentira\n",
      "mentira madre\n",
      "madre cabeza\n",
      "cabeza familia\n",
      "familia Sisben\n",
      "Sisben Haci\n",
      "Haci ayuda\n",
      "ayuda personas\n",
      "personas Sisben\n",
      "Sisben localizavan\n",
      "localizavan forma\n",
      "forma urge\n",
      "urge ayuda\n",
      " se침ora\n",
      "se침ora alcaldesa\n",
      "alcaldesa apoyo\n",
      "apoyo emergencia\n",
      "emergencia 游땞agradeceria\n",
      "游땞agradeceria cuenta\n",
      "cuenta puntaje\n",
      "puntaje sisben\n",
      "sisben nivel\n",
      "nivel sra\n",
      "sra claudia\n",
      "claudia bendiciones\n",
      "bendiciones Dios\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "tagger=\"/Users/cristianmartinez/Documents/Anaconda/Twitter/stanford-postagger-full-2018-10-16/models/spanish.tagger\"\n",
    "jar=\"/Users/cristianmartinez/Documents/Anaconda/Twitter/stanford-postagger-full-2018-10-16/stanford-postagger.jar\"\n",
    "\n",
    "spanish_postagger = StanfordPOSTagger(tagger,jar, encoding='utf8')\n",
    "\n",
    "\n",
    "sentences = ['@ClaudiaLopez @infopresidencia @Bogota La verdad esa ayuda es una  mentira soy madre cabeza de familia no aparezco en el Sisben y a칰n Haci no he resubido ning칰n ayuda se supo e que las personas que no aparecen en el Sisben las localizavan de otra forma @ClaudiaLopez me urge una ayuda','@ClaudiaLopez @infopresidencia @Bogota no se se침ora alcaldesa si podamos contar con si apoyo ya q debido a esta emergencia mundial no he podido trabajar 游땞agradeceria nos tenga en cuenta mi puntaje en el sisben es 18.47 y mi nivel es 1 quedo atenta a alg칰n recibido sra claudia bendiciones y q mi Dios la proteja']\n",
    "\n",
    "for sent in sentences:\n",
    "\n",
    "    words = sent.split()\n",
    "    tagged_words = spanish_postagger.tag(words)\n",
    "\n",
    "    nouns = []\n",
    "    palabra_anterior = ''\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag in good_pos:\n",
    "            if word[0:1] != '@':\n",
    "#                print(word+' '+tag)\n",
    "                pareja = palabra_anterior + ' ' +word\n",
    "                palabra_anterior = word\n",
    "                nouns.append(pareja)\n",
    "                print(pareja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
